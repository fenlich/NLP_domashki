{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (можно брать только часть текста, если считается слишком долго). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг \\<start>  \n",
    "    - можете использовать тот же подход с матрицей вероятностей, но по строкам хранить биграмы, а по колонкам униграммы \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)\n",
    "    - у вас будут словари с индексами биграммов и униграммов, не перепутайте их при переводе индекса в слово - словарь биграммов будет больше словаря униграммов и все индексы из униграммного словаря будут формально подходить для словаря биграммов (не будет ошибки при id2bigram[unigram_id]), но маппинг при этом будет совершенно неправильным "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ca1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix, csr_matrix, csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f134994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = open('lenta.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4188b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71acc003",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news[:5000000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a64e4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_news_train, sents_news_test = train_test_split(sentences_news, test_size=30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ed38d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(0, len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aae7224",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sents_news_train:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence, n=2))\n",
    "    trigrams_news.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbb2b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                          len(unigrams_news)))\n",
    "\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "id2bigram_news = list(bigrams_news)\n",
    "bigram2id_news = {word:i for i, word in enumerate(id2bigram_news)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5d7762",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in trigrams_news:\n",
    "    bigram, word = ngram.rsplit(' ', 1)\n",
    "    matrix_news[bigram2id_news[bigram], word2id_news[word]] =  (trigrams_news[ngram]/\n",
    "                                                                     bigrams_news[bigram])\n",
    "    \n",
    "matrix_news = csc_matrix(matrix_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0f91efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, bigram2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    previous = ['<start>']\n",
    "    \n",
    "    for i in range(n):\n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
    "        chosen_w = id2word[chosen]\n",
    "        text.append(chosen_w)\n",
    "        \n",
    "        if chosen_w == '<end>':\n",
    "            previous = ['<start>']\n",
    "            chosen_w = '<start>'\n",
    "            \n",
    "        previous.append(chosen_w)    \n",
    "        current_idx = bigram2id[' '.join(previous)]\n",
    "        previous = previous[1:]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b685d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "если закон будет принят потребители в случае победы на выборах \n",
      " об этом сегодня сообщил корреспонденту авн что поскольку проверка пока продолжается mars polar lander \n",
      " в принципе считает лужков вариант проведения досрочных президентских выборов кажется достаточно реальной \n",
      " кавказский след считается маловероятным \n",
      " по информации главной военной прокуратурой возбуждено уголовное дело по поводу адекватности правительственных мер \n",
      " крайне сложной и тревожной абсолютно криминальной назвал лидер яблока григорий явлинский \n",
      " коснувшись развернутой в западной европе приближается к миллиарду долларов \n",
      " в его распоряжении есть пленки с записями разговоров которые сделали сами киллеры которые исполняли покушение \n",
      " сейчас у нас будет\n",
      "2\n",
      "как отмечается в сообщении кпу повторные президентскиевыборы были проведены широкомасштабные учения “ антитеррор-99 ” для проверки были опрошены все пятеро категорически отказались назвать конкретные города и области могли начаться аварийные автоматические отключения \n",
      " как сообщает агентство итар-тасс из дели \n",
      " по словам черемушкина практически о том чтобы продолжать обманываться утверждая что она никогда не комментируем подобные сообщения заявил сотрудник network associates вирус не проживет а то что пока конкретных договоренностей между сторонами \n",
      " командование грвз категорически отвергает односторонний запрет на российские штыки то это должно прозойти в течение последнего времени \n",
      " потери российских войск появилась на ней будут обсуждаться проблемы\n",
      "3\n",
      "после оглашения решения суда правоохранительные органы петербурга в камере \n",
      " в последнее время неоднократно упоминалось в связи с тем генеральный секретарь парижского клуба франсуа перол клуб впредь будет соблюдать все обязательства и произведет компенсацию потерь понесенных в ходе переговоров с боевиками в городе мидлсбро упавшее дерево разломило пополам еще одну должность генерального секретаря нато \n",
      " в настоящий момент в новом блоке \n",
      " оба уроженцы грозного \n",
      " минтранс рф проверил готовность компьютерных систем объектов транспортного комплекса к переходу в следующее тысячелетие \n",
      " однако как сообщает корпорация microsoft работала над диссертацией \n",
      " объявленные итоги выборов будут известны в конце прошлой недели талибы\n",
      "4\n",
      "в тот же срок в прошлую пятницу московский суд обязал банк россии говорится в опубликованном в четверг new york times \n",
      " на высотах от 10 до 3500 метров и сильными ливнями \n",
      " заведено уголовное дело по иску кинокомпаний будет один президент во втором чтении проект федерального закона о борьбе с экономическими преступлениями генерал-майора милиции кузьму шаленкова \n",
      " это было первое выступление американского президента является развитие услуг с тем министр обороны сша подвергся компьютерной атаке \n",
      " напомним что ужесточение порядка регистрации граждан на завтра заключались по курсам максимальный 28,22 минимальный 27,8310 рубля за литр алкогольную продукцию которые в 6 000 долларов\n",
      "5\n",
      "также увеличена численность аппарата следственного комитета при мвд венгрии однако оно было подтянуто чтобы оказать давление на судей в ходе боевых действий в районе южного полюса в декабре прошлого года первоначально эксперты ожидали прибыль за iii квартал снизятся под усиливающимся давлением конкурентов и похищения с целью лишить медиа-мост независимости \n",
      " однако в конце сентября итера пошла навстречу пожеланиям грузии и молдавии официциально не ставили перед москвой ее западными партнерами о продаже заводов выпускающих грузовики и автобусы \n",
      " в конце концов начали бороться \n",
      " подобная картина наблюдается во многих областях и в частности назвал высказывание коржакова потоком сознания психически неуравновешенного человека\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    print(i)\n",
    "    print(generate(matrix_news, id2word_news, bigram2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "959227d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(logp, N):\n",
    "    return np.exp((-1/N) * logp)\n",
    "\n",
    "\n",
    "def compute_join_proba_markov_assumption(text, bigram_counts, trigram_counts):\n",
    "    prob = 0\n",
    "    for trigram in ngrammer(text, 3):\n",
    "        bigram, word = ngram.rsplit(' ', 1)\n",
    "        if bigram in bigram_counts and trigram in trigram_counts:\n",
    "            prob += np.log(trigram_counts[trigram]/bigram_counts[bigram])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return prob, len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "669f059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = []\n",
    "for sent in sents_news_test:\n",
    "    prob, N = compute_join_proba_markov_assumption(sent, bigrams_news, trigrams_news)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps.append(perplexity(prob, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9d19e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2841.8666435303926"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733858c",
   "metadata": {},
   "source": [
    "Измените функцию generate_with_beam_search так, чтобы она работала с моделью, которая учитывает два предыдущих слова. \n",
    "Сравните получаемый результат с первым заданием. \n",
    "Также попробуйте начинать генерацию не с нуля (подавая \\<start> \\<start>), а с какого-то промпта. Но помните, что учитываться будут только два последних слова, так что не делайте длинные промпты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c426746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем класс чтобы хранить каждый из лучей\n",
    "class Beam:\n",
    "    def __init__(self, sequence: list, score: float):\n",
    "        self.sequence: list = sequence\n",
    "        self.score: float = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e55e5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_beam_search(matrix, id2word, bigram2id, n=100, max_beams=5, start='<start>', prompt = ''):\n",
    "    # изначально у нас один луч с заданным началом (start по дефолту)\n",
    "    initial_node = Beam(sequence=[start] * 2 + normalize(prompt), score=np.log1p(0))\n",
    "    beams = [initial_node]\n",
    "    \n",
    "    for i in range(n):\n",
    "        # делаем n шагов генерации\n",
    "        new_beams = []\n",
    "        # на каждом шаге продолжаем каждый из имеющихся лучей\n",
    "        for beam in beams:\n",
    "            # лучи которые уже закончены не продолжаем (но и не удаляем)\n",
    "            if beam.sequence[-1] == '<end>':\n",
    "                new_beams.append(beam)\n",
    "                continue\n",
    "            \n",
    "            # наша языковая модель предсказывает на основе предыдущего слова\n",
    "            # достанем его из beam.sequence\n",
    "            last_id = bigram2id[' '.join(beam.sequence[-2:])]\n",
    "            \n",
    "            # посмотрим вероятности продолжений для предыдущего слова\n",
    "            probas = matrix[last_id].toarray()[0]\n",
    "            \n",
    "            # возьмем топ самых вероятных продолжений\n",
    "            top_idxs = probas.argsort()[:-(max_beams+1):-1]\n",
    "            # top_idxs = np.random.choice(matrix.shape[1], \n",
    "            #                             size=min(max_beams, probas.astype(bool).sum()),\n",
    "            #                             p=probas, replace=False)\n",
    "            for top_id in top_idxs:\n",
    "                # иногда вероятности будут нулевые, такое не добавляем\n",
    "                if not probas[top_id]:\n",
    "                    break\n",
    "                \n",
    "                # создадим новый луч на основе текущего и варианта продолжения\n",
    "                new_sequence = beam.sequence + [id2word[top_id]]\n",
    "                # скор каждого луча это произведение вероятностей (или сумма логарифмов)\n",
    "                new_score = (beam.score + np.log1p(probas[top_id])) / len(new_sequence)\n",
    "                new_beam = Beam(sequence=new_sequence, score=new_score)\n",
    "                new_beams.append(new_beam)\n",
    "        # отсортируем лучи по скору и возьмем только топ max_beams\n",
    "        beams = sorted(new_beams, key=lambda x: x.score, reverse=True)[:max_beams]\n",
    "    \n",
    "    # в конце возвращаем самый вероятный луч\n",
    "    # best_sequence = max(beams, key=lambda x: x.score).sequence\n",
    "    sorted_sequences = sorted(beams, key=lambda x: x.score, reverse=True)\n",
    "    sorted_sequences = [\" \".join(beam.sequence) for beam in sorted_sequences]\n",
    "    return sorted_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "094eb53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> <start> как сообщили риа новости <end>',\n",
       " '<start> <start> как сообщает риа новости <end>',\n",
       " '<start> <start> об этом риа новости <end>',\n",
       " '<start> <start> об этом сообщает риа новости <end>',\n",
       " '<start> <start> об этом сообщает агентство риа новости <end>',\n",
       " '<start> <start> об этом сообщает риа новости со ссылкой на пресс-центр мвд участники совещания заслушали руководителей подразделений на приоритетных направлениях работы защите экономики от криминального влияния ” <end>',\n",
       " '<start> <start> об этом сообщает риа новости со ссылкой на пресс-центр объединенной группировки войск на северном кавказе <end>',\n",
       " '<start> <start> об этом сообщает риа новости со ссылкой на пресс-центр мвд участники совещания заслушали руководителей подразделений на территории чечни <end>',\n",
       " '<start> <start> об этом сообщает риа новости со ссылкой на пресс-центр объединенной группировки федеральных войск на северном кавказе <end>',\n",
       " '<start> <start> об этом сообщает риа новости со ссылкой на пресс-центр мвд участники совещания заслушали руководителей подразделений на приоритетных направлениях работы защите экономики от криминального влияния противодействии наркомании и борьбе с терроризмом <end>']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_beam_search(matrix_news, id2word_news, bigram2id_news, max_beams=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3940c160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> <start> в москве волгодонске и буйнакске принадлежат к экстремистскому ваххабитскому течению <end>',\n",
       " '<start> <start> в москве волгодонске и буйнакске <end>',\n",
       " '<start> <start> в москве волгодонске буйнакске и волгодонске <end>',\n",
       " '<start> <start> в москве волгодонске и буйнакске принадлежат к местной банде татарин tatarin или братва bratva <end>',\n",
       " '<start> <start> в москве и московской области <end>',\n",
       " '<start> <start> в москве с 1 января 2000 года <end>',\n",
       " '<start> <start> в москве волгодонске и буйнакске принадлежат к сикхской радикальной группировке babbar khalsa занимающейся охотой на ведьм <end>',\n",
       " '<start> <start> в москве волгодонске и буйнакске принадлежат к сикхской радикальной группировке babbar khalsa занимающейся охотой на индийских политических лидеров высшего эшелона <end>',\n",
       " '<start> <start> в москве по связям с международными финансовыми организациями <end>',\n",
       " '<start> <start> в москве волгодонске и буйнакске принадлежат к сикхской радикальной группировке babbar khalsa занимающейся охотой на индийских политических лидеров стран нато в европе <end>']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_beam_search(matrix_news, id2word_news, bigram2id_news, max_beams=10, prompt = 'В Москве')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "barbiturat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
